{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\" style=\"width:200px;height200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Relatório PPFCentral**\n",
    "Equipe Raspagem de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **O que são Biblotecas ou Módulo?**\n",
    "\n",
    "Uma biblioteca é uma coleção de módulos de script acessíveis a um programa *Python*, isto é, um pacote de códigos que está pronto no Python. Dessa forma, você pode instalar uma biblioteca que foi produzida por outra pessoa e utilizar as ferramentas dessa biblioteca para resolver os problemas que você está enfrentando.\n",
    "\n",
    "## **Bibliotecas Básicas**\n",
    "\n",
    "### Numpy\n",
    "\n",
    "**Definição:** É uma biblioteca da linguagem Python, chamada de *Numerical Python*, é uma coleção de funções e operações que ajudam a executar cálculos numéricos com facilidade. O *NumPy* oferece uma biblioteca para cálculos fáceis e rápidos.\n",
    "\n",
    "Para deixar a ideia mais clara de como funciona a biblioteca, podemos utilizar um exemplo mais prático. Suponha que você queira resolver uma equação do segundo grau. Para isto você precisa, por exemplo, ter um conhecimento das operações de soma e subtração. Tendo isso em mente, você irá atrás de um livro de matemática básica para obter tal conhecimento e conseguir executar os cálculos. Nesse exemplo, se levarmos para linguagem de programação, podemos dizer que a biblioteca *Numpy* seria o livro de matemática básica, onde se encontra o conhecimento de soma e subtração. Logo, ao utilizarmos uma função dessa biblioteca, estamos dizendo para máquina ir nesta biblioteca e pegar um certo \"conhecimento\" para executar algum cálculo.\n",
    "\n",
    "### Pandas\n",
    "\n",
    "**Definição:** É uma biblioteca da linguagem Python, utilizada para manipulação e análise de dados. A biblioteca permite ler, manipular, agregar e plotar os dados de forma simples.\n",
    "\n",
    "Para exemplificarmos esta biblioteca, podemos usar o exemplo da criação de uma matriz. Digamos que você queira criar uma matriz. Para isso, você precisa manipular certos dados para encaixá-los corretamente na matriz. O *Pandas* funcionaria como o livro que contém o conhecimento necessário para manipulação. Logo, ao executar uma função da biblioteca, estamos pedindo para a máquina acessar este \"livro\" e executar esses conhecimentos para construir a matriz corretamente.\n",
    "\n",
    "## **WebScrapping**\n",
    "\n",
    "**Definição:** É o ato de coletar dados estruturados na Web de maneira automatizada. Dessa forma, podemos chamar o *WebScrapping* de Raspagem de Dados ou Extração de Dados da Web - ambas definições refletem bem o que é feito pelo *WebScrapping*. Neste sentido, a Raspagem de Dados desempenha um papel fundamental ao ceder os dados que serão utilizados pelas bibliotecas *Numpy* ou *Pandas* e para outros fins.\n",
    "\n",
    "A prática de *Webscrapping*, é uma maneira de automatizar o processo da coleta de dados, em uma certa página para uma análise posterior. Para exemplificar este caso, suponha que queremos saber quantas ofertas do produto *Macbook air* existem na página principal do Mercado Livre. Podemos fazer isto manualmente, acessando a página e contando um a um. Porém, se quisermos economizar mais tempo e obter uma resposta com uma menor margem de erro, podemos utilizar algumas sequências de códigos que basicamente diz para máquina acessar o site e realizar esta contagem. Fazendo isso, utilizamos a capacidade de processamento da máquina para economizar tempo.\n",
    "\n",
    "## **Bibliotecas de WebScrapping**\n",
    "\n",
    "### urlib\n",
    "\n",
    "**Definição:** É uma biblioteca para acessar, ler e fazer o *parse* (que é basicamente transformar um dado de um formato para outro) de uma URL. De certa forma, é uma biblioteca que realiza o *request* de uma URL, que possibilita a extração de dados feitas pelo BeautifulSoup.\n",
    "\n",
    "### requests\n",
    "\n",
    "**Definição:** É uma biblioteca que requisita o acesso a uma URL. De certa forma, a biblioteca *requests* é considerada como *easy-to-use* ao ser comparada com a *urlib*. Apesar disso, a *urllib* é uma biblioteca que apresenta algumas funções a mais que a *requests* não apresenta.\n",
    "\n",
    "### BeautifulSoup (bs4)\n",
    "\n",
    "**Definição:** É uma biblioteca para extrair dados de HTML e arquivos XML. Neste sentido, o *bs4* é uma biblioteca que necessita de bibliotecas como a *urlib* e a *requests* para poder funcionar. No geral, tem ótimos resultados e funciona de forma eficiente.\n",
    "\n",
    "## Definição do diretório do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:56:39.710538Z",
     "start_time": "2022-04-13T03:56:39.698569Z"
    }
   },
   "outputs": [],
   "source": [
    "wd = r'C:/Users/vitor/Desktop/Education/LAMFO/Projetos/#mcti_datascience/Github/Testes de Código'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dos módulos auxiliares\n",
    "\n",
    "As bibliotecas usadas no *ppfcentral* estão listadas acima. A maioria é embutida no *python* básico, porém é necessário instalar também as bibliotecas: (\"pandas\",\"numpy\",\"bs4\",\"requests\",\"currencyconverter\",\"googletrans\",\"lxml\") que são utilizadas tanto no *ppf* quanto nos imports dos *scrappers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:56:40.380287Z",
     "start_time": "2022-04-13T03:56:39.714562Z"
    },
    "id": "-0fetOtBlPTs"
   },
   "outputs": [],
   "source": [
    "import filecmp\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from filecmp import dircmp\n",
    "from inspect import getmembers, isfunction\n",
    "from itertools import compress\n",
    "from os import walk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import automático dos módulos de Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:56:40.782845Z",
     "start_time": "2022-04-13T03:56:40.382505Z"
    },
    "id": "zW7JmqnElPT5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_func(module):\n",
    "\n",
    "    exec('import scrappers.' + module, globals())\n",
    "    exec('funcs = getmembers(scrappers.' + module + ', isfunction)', globals())\n",
    "\n",
    "    return [f[0] for f in funcs if module.lower() in f[0].lower()]\n",
    "\n",
    "\n",
    "os.chdir(wd)\n",
    "name_scrappers = [\n",
    "    i[:-3] for i in os.listdir(wd + '\\scrappers') if i[-3:] == '.py'\n",
    "]\n",
    "funs = []\n",
    "for i in name_scrappers:\n",
    "    exec('from scrappers.' + i + ' import *')\n",
    "    for f in get_func(i):\n",
    "        funs.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável *name_scrappers* é uma lista com o nome de todos os arquivos que estão contidos na pasta *scrappers* do diretório do projeto. A função *get_func()* recebe o nome de um desses arquivos e retorna as funções de scrapping que aquele arquivo possui. Um exemplo de chamada desta função com o módulo *speciesconservation* retorna o seguinte resultado: *get_func('speciesconservation')* = {{get_func('speciesconservation')}}. A legenda utilizada para cada tipo de função é a seguinte:\n",
    "\n",
    "1.  Oportunidades\n",
    "2.  Notícias\n",
    "3.  Políticas\n",
    "4.  Projetos\n",
    "\n",
    "O loop *'for'* utilizado no código faz o import de todos os módulos de *scrapping* usando a função imbutida no python *exec()*. Depois disso ele chama a função *get_func* e guarda todas as funções de scrapping que precisam ser chamadas na lista *'funs'*. Uma vantagem de fazer os imports dos módulos desta maneira é que economizamos centenas de linhas de código que seriam necessárias para fazer os *imports* por extenso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remover scrappers com problema\n",
    "\n",
    "Algumas funções de *scrapping* apresentaram problemas que serão corrigidos futuramente com o auxílio do MCTI, por isso iremos inserir um bloco de código capaz de remover essas funções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:56:40.814094Z",
     "start_time": "2022-04-13T03:56:40.799626Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "problematicos = [\n",
    "    'fonplata1',\n",
    "    'iadb2',\n",
    "    'itto1',\n",
    "    'erefdn4',\n",
    "    'forskningsradet1'\n",
    "]\n",
    "\n",
    "for p in problematicos:\n",
    "    funs.remove(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo o diretório para salvar os arquivos\n",
    "\n",
    "Neste bloco de código nós definimos a pasta que receberá as bases de dados que resultarem das funções de *scrapping*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:56:40.830564Z",
     "start_time": "2022-04-13T03:56:40.815325Z"
    },
    "id": "kkeAvLQtlPT_"
   },
   "outputs": [],
   "source": [
    "os.chdir(wd + r'/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzY9X47klPUA"
   },
   "source": [
    "# Criação da pasta diária\n",
    "\n",
    "A coleta de dados no *ppfcentral* acontece de forma contínua, todos os dias as funções são chamadas e caso encontrem atualizações na base de dados elas salvam essas alterações na pasta diária. A pasta *baseprincipal* funciona como um *\"hard drive\"* destas informações e guarda as informações que foram coletadas nos períodos anteriores. O bloco de código à seguir é responsável pela criação do diretório das informações coletadas no dia que o código for rodado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:56:40.849247Z",
     "start_time": "2022-04-13T03:56:40.833248Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPnEvjdYlPUB",
    "outputId": "2a51ce1c-161d-4d6b-91f2-a55d098596e7"
   },
   "outputs": [],
   "source": [
    "dia = datetime.today().strftime('%y%m%d')  # yy/mm/dd\n",
    "if os.path.exists(dia):\n",
    "    print('Diretório já existente')  # nada acontece\n",
    "else:\n",
    "    os.makedirs(dia)  # cria o diretório\n",
    "    print('Diretório criado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição de termos chave para automarizar a classificação\n",
    "\n",
    "Esta é a primeira tentativa de automatizar a classificação das oportunidades de acordo com a possibilidade de participação de cientistas brasileiros. Várias oportunidades não explicitam se o Brasil está ou não incluído na lista de países elegíveis, então definimos algumas palavras-chave que podem sugerir a inclusão do Brasil. Caso o computador encontre essas palavras nos textos referentes àquela oportunidade ele irá classificar aquele ponto de dado como 'Y', ou elegível. Futuramente modelos de inteligência Artificial (IA) serão desenvolvidos para melhorar a acurácia dessa classificação.#### Definição de termos chave para automarização da classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:56:40.869348Z",
     "start_time": "2022-04-13T03:56:40.853925Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'brazil', 'latin america', 'underdeveloped', 'south america', 'brics',\n",
    "    'mercosul', 'portuguese', 'middle income'\n",
    "]\n",
    "\n",
    "keywords = '(' + '|'.join(keywords) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código que roda as funções de scrapping\n",
    "\n",
    "A função *call_fun()* verifica se a função que está sendo chamada é correspondente à coleta de **Oportunidades** ou não. Caso seja, ela vai passar o parâmetro *keywords*, caso não seja ela somente chama a função usando a função imbutida *eval()*. O loop *'for'* itera por todos os itens da lista *funs* e vai salvando os dados coletados da internet na pasta diária definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:27.562013Z",
     "start_time": "2022-04-13T03:56:40.872234Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JC_tX_QllPUE",
    "outputId": "740a74f4-c0d6-434c-b286-837c28bda7a5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def call_fun(fun, path):\n",
    "\n",
    "    global keywords\n",
    "    # print(f'Início de {fun}') # esta linha de código foi comentada para deixar o relatório mais sucinto, é recomendável\n",
    "    # deixar ela ativa para saber exatamente qual função que deu problema.\n",
    "    if f[-1] != '1':\n",
    "        text = fun + f\"('{path}')\"\n",
    "        text = text.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    else:\n",
    "        text = fun + f\"('{path}', '{keywords}')\"\n",
    "        text = text.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    return eval(text)\n",
    "\n",
    "\n",
    "try:\n",
    "    for f in funs:\n",
    "        call_fun(f, '.\\\\' + dia)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro na extração, verificar arquivo fonte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função que atualiza a base\n",
    "\n",
    "Essa função pega o arquivo do *\"hard drive\"*, que é o diretório *baseprincipal*, e adiciona ao *csv* existentes as novas informações que foram coletadas naquele dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:23:50.738254Z",
     "start_time": "2022-04-13T04:23:50.724249Z"
    },
    "id": "MNKatTKGlPUI"
   },
   "outputs": [],
   "source": [
    "def atualizador(baseprincipal, diamaisrecente):\n",
    "    diario = pd.read_csv(diamaisrecente)\n",
    "    main = pd.read_csv(baseprincipal)\n",
    "    # checando o que do 'diario' está no 'main'\n",
    "    a = diario['link'].isin(main['link'])  # usar o tag/id criado inves do link\n",
    "    b = [not bool\n",
    "         for bool in a]  # inverter para o TRUE ser a linha que não tem no main\n",
    "    novaslinhas = diario[b]\n",
    "    main = main.append(novaslinhas, ignore_index=True)  # novo main\n",
    "    main.to_csv(baseprincipal, index=False, sep=\",\")\n",
    "    #print('A base foi atualizada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhj7jww3lPUJ"
   },
   "source": [
    "# Pegando todos os diretórios da pasta output\n",
    "\n",
    "Esse bloco de código acessa o diretório que contém os arquivos da base e os organiza em uma lista para ser usada nas próximas células do script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:27.588521Z",
     "start_time": "2022-04-13T04:21:27.579378Z"
    },
    "id": "BGT-sQ46lPUJ"
   },
   "outputs": [],
   "source": [
    "_, dirnames, _ = next(walk(wd + r'/output'))\n",
    "# Os obj 0 vai ser a base principal. o 1 vai ser o dia mais recente e o 2 o dia anterior.\n",
    "dirnames.sort(reverse=True)\n",
    "# Arquivos extraídos no dia.\n",
    "filenamesDia = next(walk(wd + r'/output'+'/'+dia))[-1]\n",
    "# Arquivos extraídos na base.\n",
    "filenamesBase = next(walk(wd + r'/output'+'/baseprincipal'))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOTIGBgulPUK"
   },
   "source": [
    "# Se o arquivo não estiver na base\n",
    "\n",
    "Essa parte do código copia os arquivos que estão na pasta diária e não estão na base principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:27.604631Z",
     "start_time": "2022-04-13T04:21:27.592570Z"
    },
    "id": "iRizprT8lPUK"
   },
   "outputs": [],
   "source": [
    "# Verificando se os nomes que estão no Dia estão na Base.\n",
    "a = [i in filenamesBase for i in filenamesDia]\n",
    "a = [not bool for bool in a]  # inversão pro True ser o arquivo faltante\n",
    "# arquivo que está faltando na baseprincipal\n",
    "arquivos = list(compress(filenamesDia, a))\n",
    "for f in arquivos:\n",
    "    shutil.copy('.\\\\'+dia+'\\\\'+f, '.\\\\baseprincipal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhbJpcEGlPUL"
   },
   "source": [
    "# Atualização\n",
    "\n",
    "Essa parte aplica as funções definidas nos blocos de código anteriores e efetivamente concatena as novas informações que estão localizadas nos diretórios diários aos *dataframes* que estão localizados na *baseprincipal*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:23:56.276456Z",
     "start_time": "2022-04-13T04:23:53.325086Z"
    },
    "id": "T7x2bPqVlPUL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def paths(pasta, arquivo):\n",
    "    path = '''.\\\\''' + pasta + '''\\\\''' + arquivo\n",
    "    return (path)\n",
    "\n",
    "\n",
    "filenames = filenamesDia\n",
    "\n",
    "for i in range(0, len(filenames)):\n",
    "    try:\n",
    "        base = str(paths(dirnames[0], filenames[i]))  # base\n",
    "        dia1 = paths(dirnames[1], filenames[i])  # dia mais recente\n",
    "        dia2 = paths(dirnames[2], filenames[i])  # dia anterior\n",
    "        comp = filecmp.cmp(dia1, dia2, shallow=False)\n",
    "        if comp == False:\n",
    "            #print('arquivos diários são diferentes, base atualizada')\n",
    "            atualizador(base, dia1)\n",
    "        else:\n",
    "            None\n",
    "            #print('arquivos diários são iguais, base não atualizada')\n",
    "    except:\n",
    "        None\n",
    "        #print(\"Arquivo do dia anterior não encontrado\")\n",
    "\n",
    "    #print('Concluido arquivo '+str(i+1))\n",
    "\n",
    "# como a coluna codigo contem o dia, os arquivos sempre vão ser diferentes.\n",
    "\n",
    "# As linhas de código que jogam na tela a situação da atualização da base foi comentadada para deixar o relatório mais sucinto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi5cUZbblPUL"
   },
   "source": [
    "# Criação das Bases Aumentadas\n",
    "\n",
    "Cada função de *scrapping* criada salva um arquivo csv na base principal, então é necessário criar um bloco de código que junta essas centenas de arquivos em um só para termos nossa base de dados completa. Cada tipo de informação (oportunidades, notícias, políticas e projetos) possui um padrão de variáveis que é explicitado no arquivo README deste projeto. Por esse motivos é possível utilizar a função *pd.concat()* para juntar essas variáveis e concluir a base de dados final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:32.406449Z",
     "start_time": "2022-04-13T04:21:30.498556Z"
    },
    "id": "a5oLZdWXlPUM"
   },
   "outputs": [],
   "source": [
    "# Criar dataframes\n",
    "opo_baseprincipal = pd.concat([pd.read_csv(wd + r'/output'+'/baseprincipal/'+i)\n",
    "                              for i in [i for i in filenamesBase if i[-6:] == '01.csv']])\n",
    "not_baseprincipal = pd.concat([pd.read_csv(wd + r'/output'+'/baseprincipal/'+i)\n",
    "                              for i in [i for i in filenamesBase if i[-6:] == '02.csv']])\n",
    "pol_baseprincipal = pd.concat([pd.read_csv(wd + r'/output'+'/baseprincipal/'+i)\n",
    "                              for i in [i for i in filenamesBase if i[-6:] == '03.csv']])\n",
    "prj_baseprincipal = pd.concat([pd.read_csv(wd + r'/output'+'/baseprincipal/'+i)\n",
    "                              for i in [i for i in filenamesBase if i[-6:] == '04.csv']])\n",
    "\n",
    "# Salvamento das bases em folder específico\n",
    "opo_baseprincipal.to_csv(\n",
    "    wd + r'/output'+'/baseprincipal/basescompletas/oportunidades.csv', index=False)\n",
    "not_baseprincipal.to_csv(\n",
    "    wd + r'/output'+'/baseprincipal/basescompletas/noticias.csv', index=False)\n",
    "pol_baseprincipal.to_csv(\n",
    "    wd + r'/output'+'/baseprincipal/basescompletas/politicas.csv', index=False)\n",
    "prj_baseprincipal.to_csv(\n",
    "    wd + r'/output'+'/baseprincipal/basescompletas/projetos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado Esperado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após rodar o código acima quatro arquivos no formato .csv devem ter sido criados na pasta *'basescompletas'* dentro da pasta *'baseprincipal'*. Exemplos de como cada um destes arquivos estão formatados serão incluídos abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oportunidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:32.452438Z",
     "start_time": "2022-04-13T04:21:32.408463Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opo_baseprincipal.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notícias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:32.484047Z",
     "start_time": "2022-04-13T04:21:32.453265Z"
    }
   },
   "outputs": [],
   "source": [
    "not_baseprincipal.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Políticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:32.527033Z",
     "start_time": "2022-04-13T04:21:32.488035Z"
    }
   },
   "outputs": [],
   "source": [
    "pol_baseprincipal.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projetos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:21:32.565152Z",
     "start_time": "2022-04-13T04:21:32.532020Z"
    }
   },
   "outputs": [],
   "source": [
    "prj_baseprincipal.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ppfcentral2 - multiprocessing.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "311.844px",
    "left": "906px",
    "right": "20px",
    "top": "85px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
